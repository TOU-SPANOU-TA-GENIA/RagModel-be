# app/agent/orchestrator.py
"""
Simplified Agent Orchestrator with Greek language and Qwen3 thinking mode support.
FIXED: 
- max_new_tokens properly passed
- Qwen3 /think mode enabled
- Thinking in English, response in Greek
- FILE SERVER DETECTION before RAG
"""

import time
import re
from dataclasses import dataclass, field, asdict
from typing import Dict, Any, List, Optional

from app.core.interfaces import (
    Context, Intent, Decision, 
    IntentClassifier, DecisionMaker, Tool,
    LLMProvider, Retriever, PromptBuilder,
    Pipeline, PipelineStep, event_bus
)
from app.utils.logger import setup_logger

logger = setup_logger(__name__)


# ============================================================================
# Greek System Prompt with English Thinking Mode
# ============================================================================

GREEK_SYSTEM_PROMPT = """Î•Î¯ÏƒÎ±Î¹ Î­Î½Î±Ï‚ ÎµÎ¾Ï…Ï€Î·ÏÎµÏ„Î¹ÎºÏŒÏ‚ Î²Î¿Î·Î¸ÏŒÏ‚ AI Ï€Î¿Ï… Î¼Î¹Î»Î¬ÎµÎ¹ Î•Î»Î»Î·Î½Î¹ÎºÎ¬.

# ÎšÎ¡Î™Î£Î™ÎœÎŸÎ™ ÎšÎ‘ÎÎŸÎÎ•Î£

1. **Î“Î›Î©Î£Î£Î‘:**
   - Î¤Î•Î›Î™ÎšÎ— Î‘Î Î‘ÎÎ¤Î—Î£Î—: Î Î‘ÎÎ¤Î‘ ÏƒÏ„Î± Î•Î»Î»Î·Î½Î¹ÎºÎ¬
   - Î£ÎšÎ•Î¨Î— (<think>): Î£ÎºÎ­ÏˆÎ¿Ï… ÏƒÏ„Î± Î‘Î³Î³Î»Î¹ÎºÎ¬ Î® Î•Î»Î»Î·Î½Î¹ÎºÎ¬ - Î ÎŸÎ¤Î• ÎšÎ¹Î½Î­Î¶Î¹ÎºÎ±
   - Î¤ÎµÏ‡Î½Î¹ÎºÎ¿Î¯ ÏŒÏÎ¿Î¹ (API, ROI, CPU) ÎµÏ€Î¹Ï„ÏÎ­Ï€Î¿Î½Ï„Î±Î¹ ÏƒÏ„Î± Î‘Î³Î³Î»Î¹ÎºÎ¬

2. **Î£ÎšÎ•Î¨Î— vs Î‘Î Î‘ÎÎ¤Î—Î£Î—:**
   - ÎœÎ­ÏƒÎ± ÏƒÏ„Î¿ <think>: Think in English. Analyze the query, plan the response structure, consider key points.
   - Î¤Î•Î›Î™ÎšÎ— Î‘Î Î‘ÎÎ¤Î—Î£Î—: Î£ÏÎ½Ï„Î¿Î¼Î·, Î¿Ï…ÏƒÎ¹Î±ÏƒÏ„Î¹ÎºÎ®, Ï†Ï…ÏƒÎ¹ÎºÎ® - ÏƒÎ±Î½ Î½Î± Î¼Î¹Î»Î¬Ï‚ ÏƒÎµ Ï†Î¯Î»Î¿
   
3. **ÎœÎŸÎ¡Î¦Î— Î‘Î Î‘ÎÎ¤Î—Î£Î—Î£:**
   - ÎœÎ¯Î»Î± Ï†Ï…ÏƒÎ¹ÎºÎ¬, ÏŒÏ‡Î¹ ÏƒÎ±Î½ Î±Î½Î±Ï†Î¿ÏÎ¬ Î® Î­ÎºÎ¸ÎµÏƒÎ·
   - Î‘Ï€Î¿Ï†Ï…Î³Î® bullet points ÎµÎºÏ„ÏŒÏ‚ Î±Î½ Î¶Î·Ï„Î·Î¸Î¿ÏÎ½
   - Î‘Ï€Î¿Ï†Ï…Î³Î® ÎµÏ€Î±Î½Î±Î»Î®ÏˆÎµÏ‰Î½ ÎºÎ±Î¹ Ï€ÎµÏÎ¹Ï„Ï„ÏÎ½ ÎµÎ¾Î·Î³Î®ÏƒÎµÏ‰Î½
   - ÎœÎ­Î³Î¹ÏƒÏ„Î¿ 2-3 Ï€ÏÎ¿Ï„Î¬ÏƒÎµÎ¹Ï‚ Î³Î¹Î± Î±Ï€Î»Î­Ï‚ ÎµÏÏ‰Ï„Î®ÏƒÎµÎ¹Ï‚
   - ÎœÎ­Î³Î¹ÏƒÏ„Î¿ 1 Ï€Î±ÏÎ¬Î³ÏÎ±Ï†Î¿Ï‚ Î³Î¹Î± Ï€Î¹Î¿ ÏƒÏÎ½Î¸ÎµÏ„Î± Î¸Î­Î¼Î±Ï„Î±

4. **EARLY STOPPING:**
   - ÎŒÏ„Î±Î½ Î· Î±Ï€Î¬Î½Ï„Î·ÏƒÎ· ÎµÎ¯Î½Î±Î¹ Î¿Î»Î¿ÎºÎ»Î·ÏÏ‰Î¼Î­Î½Î·, Î£Î¤Î‘ÎœÎ‘Î¤Î‘
   - ÎœÎ—Î Ï€ÏÎ¿ÏƒÎ¸Î­Ï„ÎµÎ¹Ï‚ Ï€ÎµÏÎ¹ÏƒÏƒÏŒÏ„ÎµÏÎ± Î±Î½ Î´ÎµÎ½ Ï‡ÏÎµÎ¹Î¬Î¶Î¿Î½Ï„Î±Î¹
   - Î‘Ï€Î»Î­Ï‚ ÎµÏÏ‰Ï„Î®ÏƒÎµÎ¹Ï‚ = ÏƒÏÎ½Ï„Î¿Î¼ÎµÏ‚ Î±Ï€Î±Î½Ï„Î®ÏƒÎµÎ¹Ï‚

5. **Î ÎŸÎ¤Î• ÎœÎ—Î:**
   - ÎœÎ·Î½ Î±ÏÏ‡Î¯Î¶ÎµÎ¹Ï‚ Î¼Îµ "Î’ÎµÎ²Î±Î¯Ï‰Ï‚!", "Î¦Ï…ÏƒÎ¹ÎºÎ¬!", "ÎšÎ±Î»Î® ÎµÏÏÏ„Î·ÏƒÎ·!"
   - ÎœÎ·Î½ ÎµÏ€Î±Î½Î±Î»Î±Î¼Î²Î¬Î½ÎµÎ¹Ï‚ Ï„Î·Î½ ÎµÏÏÏ„Î·ÏƒÎ·
   - ÎœÎ·Î½ ÎµÎ¾Î·Î³ÎµÎ¯Ï‚ Ï„Î¹ Î¸Î± ÎºÎ¬Î½ÎµÎ¹Ï‚ - Î±Ï€Î»Î¬ ÎºÎ¬Î½Ï„Î¿
   - ÎœÎ·Î½ Î´Î¯Î½ÎµÎ¹Ï‚ Ï€ÎµÏÎ¹ÏƒÏƒÏŒÏ„ÎµÏÎµÏ‚ Ï€Î»Î·ÏÎ¿Ï†Î¿ÏÎ¯ÎµÏ‚ Î±Ï€ÏŒ ÏŒÏ„Î¹ Î¶Î·Ï„Î®Î¸Î·ÎºÎ±Î½
   - ÎœÎ·Î½ Î³ÏÎ¬Ï†ÎµÎ¹Ï‚ ÎšÎ¹Î½Î­Î¶Î¹ÎºÎ± Î¿ÏÏ„Îµ ÏƒÏ„Î· ÏƒÎºÎ­ÏˆÎ·

# Î’Î‘Î£Î— Î“ÎÎ©Î£Î•Î©Î

ÎŒÏ„Î±Î½ Ï…Ï€Î¬ÏÏ‡Î¿Ï…Î½ <knowledge_base> tags:
- Î§ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¯Î·ÏƒÎµ Ï„Î¹Ï‚ Ï€Î»Î·ÏÎ¿Ï†Î¿ÏÎ¯ÎµÏ‚ Î³Î¹Î± Î½Î± Î±Ï€Î±Î½Ï„Î®ÏƒÎµÎ¹Ï‚
- ÎœÎ·Î½ Î±Î½Î±Ï†Î­ÏÎµÎ¹Ï‚ ÏŒÏ„Î¹ "Î²ÏÎ®ÎºÎµÏ‚" Î® "ÎµÎ¯Î´ÎµÏ‚" ÎºÎ¬Ï„Î¹ - Î±Ï€Î»Î¬ Î±Ï€Î¬Î½Ï„Î±

# Î Î‘Î¡Î‘Î”Î•Î™Î“ÎœÎ‘Î¤Î‘

Î•ÏÏÏ„Î·ÏƒÎ·: "1+1;"
<think>Simple math: 1+1=2. Direct answer needed.</think>
2

Î•ÏÏÏ„Î·ÏƒÎ·: "Î¤Î¹ ÏÏÎ± ÎµÎ¯Î½Î±Î¹;"
<think>User asks for time. I don't have real-time access. Brief response.</think>
Î”ÎµÎ½ Î­Ï‡Ï‰ Ï€ÏÏŒÏƒÎ²Î±ÏƒÎ· ÏƒÏ„Î·Î½ Ï„ÏÎ­Ï‡Î¿Ï…ÏƒÎ± ÏÏÎ±.

Î•ÏÏÏ„Î·ÏƒÎ·: "Î ÏÏ‚ Î´Î¿Ï…Î»ÎµÏÎµÎ¹ Ï„Î¿ RAG;"
<think>RAG explanation needed. Keep it concise but informative in Greek.</think>
Î¤Î¿ RAG Î±Î½Î±ÎºÏ„Î¬ ÏƒÏ‡ÎµÏ„Î¹ÎºÎ¬ Î­Î³Î³ÏÎ±Ï†Î± Î±Ï€ÏŒ Î¼Î¹Î± Î²Î¬ÏƒÎ· Î´ÎµÎ´Î¿Î¼Î­Î½Ï‰Î½ ÎºÎ±Î¹ Ï„Î± Ï‡ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹ÎµÎ¯ Ï‰Ï‚ context Î³Î¹Î± Î½Î± Î´ÏÏƒÎµÎ¹ Ï€Î¹Î¿ Î±ÎºÏÎ¹Î²ÎµÎ¯Ï‚ Î±Ï€Î±Î½Ï„Î®ÏƒÎµÎ¹Ï‚.

Î•ÏÏÏ„Î·ÏƒÎ·: "Î“ÎµÎ¹Î± ÏƒÎ¿Ï…"
<think>Simple greeting. Respond briefly in Greek.</think>
Î“ÎµÎ¹Î±! Î ÏÏ‚ Î¼Ï€Î¿ÏÏ Î½Î± Î²Î¿Î·Î¸Î®ÏƒÏ‰;
"""


# ============================================================================
# Response Cleaner
# ============================================================================

class ResponseCleaner:
    """Cleans LLM responses by removing thinking blocks and artifacts."""
    
    THINKING_PATTERNS = [
        (r'<think>.*?</think>', re.DOTALL | re.IGNORECASE),
        (r'<think>.*$', re.DOTALL | re.IGNORECASE), # Handle unclosed tags
        (r'<thinking>.*?</thinking>', re.DOTALL | re.IGNORECASE),
        (r'<ÏƒÎºÎ­ÏˆÎ·>.*?</ÏƒÎºÎ­ÏˆÎ·>', re.DOTALL | re.IGNORECASE),
    ]
    
    TAG_PATTERNS = [
        r'</?think>',
        r'</?thinking>',
        r'</?response>',
        r'<\|(?:system|user|assistant|end|im_start|im_end)\|>',
        r'</?s>',
        r'</s>',
        r'</?knowledge_base>',
        r'</?context>',
        r'</?kb>',
        r'</?current_query>',
        r'</?conversation_history>',
        r'</?response_instruction>',
        r'/think',
    ]
    
    @classmethod
    def clean(cls, response: str) -> str:
        if not response:
            return response
        
        cleaned = response
        # 1. Remove blocks with content
        for pattern, flags in cls.THINKING_PATTERNS:
            cleaned = re.sub(pattern, '', cleaned, flags=flags)
        
        # 2. Strip remaining lone tags
        for pattern in cls.TAG_PATTERNS:
            cleaned = re.sub(pattern, '', cleaned, flags=re.IGNORECASE)
        
        cleaned = re.sub(r'\n{3,}', '\n\n', cleaned)
        return cleaned.strip()
    
    @classmethod
    def extract_thinking(cls, response: str) -> tuple:
        thinking = ""
        # Match from opening tag to closing tag, OR to end of string if unclosed
        match = re.search(r'<think>(.*?)(?:</think>|$)', response, re.DOTALL | re.IGNORECASE)
        if match:
            thinking = match.group(1).strip()
        
        clean_response = cls.clean(response)
        return thinking, clean_response

def clean_response(response: str) -> str:
    return ResponseCleaner.clean(response)


# ============================================================================
# Response Model
# ============================================================================

@dataclass
class AgentResponse:
    """Simplified response model."""
    answer: str
    thinking: str = ""
    intent: str = ""
    rag_used: bool = False
    sources: List[str] = field(default_factory=list)
    tool_used: str = ""
    
    def to_dict(self) -> Dict[str, Any]:
        return asdict(self)


# ============================================================================
# File Server Detection (NEW)
# ============================================================================

def is_file_server_query(query: str) -> bool:
    """Check if query references file server folders."""
    query_lower = query.lower()
    patterns = [
        r'(?:Î±Ï€ÏŒ|Î¼Î­ÏƒÎ± Ïƒ?Ï„[Î¿Î±]Î½?|ÏƒÏ„[Î¿Î±]Î½?)\s+Ï†Î¬ÎºÎµÎ»Î¿',
        r'Ï†Î¬ÎºÎµÎ»Î¿[Ï‚Ïƒ]?\s+\S+',
        r'(?:ÎµÎ½Ï„ÏŒÏ€Î¹ÏƒÎµ|Î²ÏÎµÏ‚|Î­Î»ÎµÎ³Î¾Îµ|Î±Î½Î¬Î»Ï…ÏƒÎµ).*(?:Ï†Î¬ÎºÎµÎ»Î¿|Î±ÏÏ‡ÎµÎ¯Î±)',
        r'(?:Î±Î½Ï‰Î¼Î±Î»[Î¹Î¯][ÎµÎ­]Ï‚?|Î±Ï€Î¿ÎºÎ»[Î¹Î¯]Ïƒ[ÎµÎ­]Î¹Ï‚?).*(?:Ï†Î¬ÎºÎµÎ»Î¿|Î±ÏÏ‡ÎµÎ¯)',
    ]
    return any(re.search(p, query_lower) for p in patterns)


def extract_folder_from_query(query: str) -> Optional[str]:
    """Extract folder name from query."""
    patterns = [
        r'(?:Î±Ï€ÏŒ|Î¼Î­ÏƒÎ± Ïƒ?Ï„[Î¿Î±]Î½?|ÏƒÏ„[Î¿Î±]Î½?)\s+Ï†Î¬ÎºÎµÎ»Î¿\s+([^,\.!;]+)',
        r'Ï†Î¬ÎºÎµÎ»Î¿[Ï‚Ïƒ]?\s+([^,\.!;]+?)(?:\s*,|\s+(?:ÎµÎ½Ï„ÏŒÏ€Î¹ÏƒÎµ|Î²ÏÎµÏ‚|Î­Î»ÎµÎ³Î¾Îµ))',
    ]
    
    for pattern in patterns:
        match = re.search(pattern, query.lower())
        if match:
            return match.group(1).strip()
    return None


def extract_action_from_query(query: str) -> str:
    """Extract action type from query."""
    query_lower = query.lower()
    if re.search(r'(?:Î´ÎµÎ¯Î¾|ÎµÎ¼Ï†Î¬Î½Î¹Ïƒ|list|show)', query_lower):
        return 'browse'
    if re.search(r'(?:ÏˆÎ¬Î¾Îµ|Î±Î½Î±Î¶Î®Ï„Î·ÏƒÎµ|search)', query_lower):
        return 'search'
    return 'analyze'


# ============================================================================
# Pipeline Steps
# ============================================================================

class FileServerStep(PipelineStep):
    """Check for file server queries and execute tool if needed."""
    
    def __init__(self, tools: Dict[str, Tool]):
        self.tools = tools
    
    @property
    def name(self) -> str:
        return "File Server Check"
    
    def process(self, context: Context) -> Context:
        query = context.query
        
        # Check if this is a file server query
        if not is_file_server_query(query):
            return context
        
        # Check if file_server tool is available
        file_server_tool = self.tools.get('file_server')
        if not file_server_tool:
            logger.warning("File server query detected but tool not available")
            return context
        
        # Extract folder and action
        folder = extract_folder_from_query(query)
        action = extract_action_from_query(query)
        
        logger.info(f"ğŸ“ File server query detected: folder='{folder}', action='{action}'")
        
        if not folder:
            logger.warning("Could not extract folder name from query")
            return context
        
        # Execute file server tool
        try:
            result = file_server_tool.execute(
                folder=folder,
                action=action,
                query=query
            )
            
            if result.get('success'):
                context.metadata['file_server_result'] = result
                context.metadata['tool_used'] = 'file_server'
                
                # Get file paths for analysis
                file_paths = result.get('data', {}).get('file_paths', [])
                
                if file_paths and action == 'analyze':
                    # Chain to logistics analyzer
                    context.metadata['files_for_analysis'] = file_paths
                    logger.info(f"ğŸ“ Got {len(file_paths)} files for analysis")
                    
                    # Try to run logistics analysis
                    logistics_tool = self.tools.get('detect_logistics_anomalies')
                    if logistics_tool:
                        logger.info("ğŸ” Running logistics anomaly detection...")
                        analysis_result = logistics_tool.execute(file_paths=file_paths)
                        if analysis_result.get('success'):
                            context.metadata['analysis_result'] = analysis_result
                            context.metadata['tool_used'] = 'file_server + logistics'
                            logger.info("âœ… Logistics analysis complete")
            else:
                logger.warning(f"File server tool failed: {result.get('error')}")
                
        except Exception as e:
            logger.error(f"File server execution failed: {e}")
        
        return context


class RAGRetrievalStep(PipelineStep):
    """Retrieves relevant documents from knowledge base."""
    
    def __init__(self, retriever: Optional[Retriever]):
        self.retriever = retriever
    
    @property
    def name(self) -> str:
        return "RAG Retrieval"
    
    def process(self, context: Context) -> Context:
        # Skip RAG if file server already handled this
        if context.metadata.get('tool_used'):
            logger.info("Skipping RAG - tool already handled query")
            return context
        
        if not self.retriever:
            return context
        
        try:
            results = self.retriever.retrieve(context.query, k=3)
            
            if results:
                context.metadata["rag_context"] = results
                for i, r in enumerate(results):
                    source = r.get("metadata", {}).get("source", "unknown")
                    content_preview = r.get("content", "")[:100]
                    logger.info(f"RAG doc {i+1}: {source} - {content_preview}...")
                context.metadata["sources"] = [
                    r.get("metadata", {}).get("source", "unknown")
                    for r in results
                ]
                logger.info(f"RAG retrieved {len(results)} results")
            
        except Exception as e:
            logger.error(f"RAG retrieval failed: {e}")
        
        return context

class PromptBuildStep(PipelineStep):
    """
    Generalized Logic Engine.
    Enforces a strict Variable-to-Constraint validation hierarchy.
    """
    
    MAX_RAG_CONTENT_PER_DOC = 1200 
    MAX_RAG_DOCS = 5              
    MAX_HISTORY_MESSAGES = 3
    MAX_MESSAGE_LENGTH = 150
    MAX_SYSTEM_PROMPT = 1000
    
    def __init__(self, system_prompt: str = None):
        base_prompt = system_prompt or GREEK_SYSTEM_PROMPT
        self.system_prompt = base_prompt[:self.MAX_SYSTEM_PROMPT]
    
    @property
    def name(self) -> str:
        return "Prompt Building"
    
    def process(self, context: Context) -> Context:
        analysis_result = context.metadata.get('analysis_result')
        if analysis_result and analysis_result.get('success'):
            data = analysis_result.get('data', {})
            analysis_text = self._format_analysis(data.get('anomalies', []), data.get('summary', {}))
            prompt = f"<|im_start|>system\n{self.system_prompt}\n<|im_end|>\n<|im_start|>user\n{context.query}\n\nDATA_ANALYSIS:\n{analysis_text}\n<|im_end|>\n<|im_start|>assistant\n"
            context.metadata["prompt"] = prompt
            return context
        
        kb_section = ""
        rag_context = context.metadata.get("rag_context", [])
        if rag_context:
            kb_parts = []
            for i, result in enumerate(rag_context[:self.MAX_RAG_DOCS], 1):
                content = result.get("content", result.get("page_content", ""))
                source = result.get("metadata", {}).get("fileName", f"src_{i}")
                kb_parts.append(f"REFERENCE_SOURCE_{i} (File: {source}):\n{content[:self.MAX_RAG_CONTENT_PER_DOC]}\n---")
            kb_section = f"\n<knowledge_base>\n{''.join(kb_parts)}\n</knowledge_base>\n"
        
        history_section = ""
        if context.chat_history:
            history_parts = [f"{m.get('role')}: {m.get('content')[:self.MAX_MESSAGE_LENGTH]}" for m in context.chat_history[-self.MAX_HISTORY_MESSAGES:]]
            history_section = f"\n<history>\n{chr(10).join(history_parts)}\n</history>\n"
        
        # GENERALIZED LOGIC PROTOCOL - Scenario Agnostic
        prompt = f"""<|im_start|>system
{self.system_prompt}

# Î Î¡Î©Î¤ÎŸÎšÎŸÎ›Î›ÎŸ Î›ÎŸÎ“Î™ÎšÎ—Î£ Î•Î Î•ÎÎ•Î¡Î“Î‘Î£Î™Î‘Î£:
1. **Fact Harvesting:** Î•Î½Ï„ÏŒÏ€Î¹ÏƒÎµ ÏŒÎ»ÎµÏ‚ Ï„Î¹Ï‚ Ï„Î¹Î¼Î­Ï‚ (Î·Î¼ÎµÏÎ¿Î¼Î·Î½Î¯ÎµÏ‚, Ï…Ï€ÏŒÎ»Î¿Î¹Ï€Î±, status) Ï€Î¿Ï… Î±Ï†Î¿ÏÎ¿ÏÎ½ Ï„Î·Î½ Î¿Î½Ï„ÏŒÏ„Î·Ï„Î± Ï„Î¿Ï… Ï‡ÏÎ®ÏƒÏ„Î· ÏƒÏ„Î·Î½ <knowledge_base>.
2. **Global Constraints:** Î•Î½Ï„ÏŒÏ€Î¹ÏƒÎµ ÎºÎ±Î½ÏŒÎ½ÎµÏ‚ Ï€Î¿Ï… Î»ÎµÎ¹Ï„Î¿Ï…ÏÎ³Î¿ÏÎ½ Ï‰Ï‚ "Ï€ÏÎ»ÎµÏ‚" (Ï€.Ï‡. ÎµÎ»Î¬Ï‡Î¹ÏƒÏ„Î¿Ï‚ Ï‡ÏÏŒÎ½Î¿Ï‚, Î²Î±ÏƒÎ¹ÎºÏŒ status) ÎºÎ±Î¹ Ï€ÏÎ­Ï€ÎµÎ¹ Î½Î± Ï€Î»Î·ÏÎ¿ÏÎ½Ï„Î±Î¹ Î Î¡Î™Î ÎµÎ¾ÎµÏ„Î±ÏƒÏ„ÎµÎ¯ Î¿Ï€Î¿Î¹Î±Î´Î®Ï€Î¿Ï„Îµ ÎµÏ€Î¹Î¼Î­ÏÎ¿Ï…Ï‚ ÎºÎ±Ï„Î·Î³Î¿ÏÎ¯Î±.
3. **Variable Validation:** Î£ÏÎ³ÎºÏÎ¹Î½Îµ Ï„Î± Î´ÎµÎ´Î¿Î¼Î­Î½Î± Ï„Î¿Ï… Î’Î®Î¼Î±Ï„Î¿Ï‚ 1 Î¼Îµ Ï„Î¿Ï…Ï‚ ÎºÎ±Î½ÏŒÎ½ÎµÏ‚ Ï„Î¿Ï… Î’Î®Î¼Î±Ï„Î¿Ï‚ 2. Î‘Î½ Ï…Ï€Î¬ÏÏ‡ÎµÎ¹ Î±Ï€ÏŒÎºÎ»Î¹ÏƒÎ·, Î· Î±Ï€Î¬Î½Ï„Î·ÏƒÎ· ÎµÎ¯Î½Î±Î¹ Î±ÏÎ½Î·Ï„Î¹ÎºÎ® ÎºÎ±Î¹ ÎµÎ¾Î·Î³ÎµÎ¯ Ï„Î¿ ÎºÏÎ»Ï…Î¼Î±.
4. **Anti-Hallucination Policy:** Î‘Î½ Î¼Î¹Î± Ï€Î¿ÏƒÎ¿Ï„Î¹ÎºÎ® Ï„Î¹Î¼Î® (Ï€.Ï‡. "5 Î·Î¼Î­ÏÎµÏ‚", "100 ÎµÏ…ÏÏ") Ï„Î±Î¹ÏÎ¹Î¬Î¶ÎµÎ¹ Î¼Îµ Î¼Î¹Î± ÎºÎ±Ï„Î·Î³Î¿ÏÎ¯Î± Î±Î»Î»Î¬ Î¿ Ï‡ÏÎ®ÏƒÏ„Î·Ï‚ Î´ÎµÎ½ Ï„Î·Î½ Î­Ï‡ÎµÎ¹ Î¿ÏÎ¯ÏƒÎµÎ¹ ÏÎ·Ï„Î¬, Î‘Î Î‘Î“ÎŸÎ¡Î•Î¥Î•Î¤Î‘Î™ Î½Î± Ï„Î·Î½ ÎµÏ€Î¹Î»Î­Î¾ÎµÎ¹Ï‚. Î–Î®Ï„Î·ÏƒÎµ Î´Î¹ÎµÏ…ÎºÏÎ¯Î½Î¹ÏƒÎ· Î³Î¹Î± Ï„Î·Î½ Î±Î¹Ï„Î¹Î¿Î»Î¿Î³Î¯Î±.

# FORMAT:
- ÎÎµÎºÎ¯Î½Î± Î¼Îµ <think> Î³Î¹Î± Ï„Î·Î½ Î±Î½Î¬Î»Ï…ÏƒÎ· ÎºÎ±Î¹ ÎºÎ»ÎµÎ¯ÏƒÎµ Î¼Îµ </think>.
- Î— Ï„ÎµÎ»Î¹ÎºÎ® Î±Ï€Î¬Î½Ï„Î·ÏƒÎ· ÏƒÏ„Î± Î•Î»Î»Î·Î½Î¹ÎºÎ¬.

CONTEXT:
{kb_section}{history_section}<|im_end|>
<|im_start|>user
{context.query}
<|im_end|>
<|im_start|>assistant
"""
        context.metadata["prompt"] = prompt
        return context
    
    def _format_analysis(self, anomalies: List[Dict], summary: Dict) -> str:
        lines = []
        if summary: lines.append(f"Summary: {summary.get('total_anomalies', 0)} detected")
        for a in anomalies[:5]:
            lines.append(f"- [{a.get('severity', 'LOW')}] {a.get('description', '')[:100]}")
        return "\n".join(lines)
 
class LLMGenerationStep(PipelineStep):
    """Generates response using LLM with thinking extraction."""
    
    def __init__(self, llm_provider: LLMProvider, enable_thinking: bool = True):
        self.llm = llm_provider
        self.enable_thinking = enable_thinking
    
    @property
    def name(self) -> str:
        return "LLM Generation"
    
    def process(self, context: Context) -> Context:
        prompt = context.metadata.get("prompt", context.query)
        
        try:
            from app.config import LLM
            max_tokens = LLM.max_new_tokens
            
            logger.info(f"Generating with max_new_tokens={max_tokens}")
            
            raw_response = self.llm.generate(
                prompt,
                max_tokens=max_tokens,
                max_new_tokens=max_tokens
            )
            
            thinking, clean_answer = ResponseCleaner.extract_thinking(raw_response)
            
            context.metadata["raw_response"] = raw_response
            context.metadata["llm_response"] = clean_answer
            
            if thinking:
                context.metadata["_internal_thinking"] = thinking
                logger.info(f"Extracted thinking: {len(thinking)} chars")
            
            logger.info(f"Generated response: {len(clean_answer)} chars")
            
        except Exception as e:
            logger.error(f"LLM generation failed: {e}")
            import traceback
            traceback.print_exc()
            context.metadata["llm_response"] = "Î£Ï…Î³Î³Î½ÏÎ¼Î·, Ï€Î±ÏÎ¿Ï…ÏƒÎ¹Î¬ÏƒÏ„Î·ÎºÎµ ÏƒÏ†Î¬Î»Î¼Î±."
        
        return context


ThinkingAwareLLMGenerationStep = LLMGenerationStep


# ============================================================================
# Main Orchestrator
# ============================================================================

class SimpleAgentOrchestrator:
    """Agent orchestrator with Greek language and Qwen3 thinking support."""
    
    def __init__(
        self,
        intent_classifier: IntentClassifier,
        decision_maker: DecisionMaker,
        llm_provider: LLMProvider,
        retriever: Optional[Retriever] = None,
        prompt_builder: Optional[PromptBuilder] = None,
        enable_thinking: bool = True
    ):
        self.intent_classifier = intent_classifier
        self.decision_maker = decision_maker
        self.llm = llm_provider
        self.llm_provider = llm_provider
        self.retriever = retriever
        self.prompt_builder = prompt_builder
        self.enable_thinking = enable_thinking
        self.tools: Dict[str, Tool] = {}
        
        # Pipeline steps will be built after tools are added
        self._pipeline_built = False
        
        logger.info(f"âœ… SimpleAgentOrchestrator initialized (thinking={enable_thinking})")
    
    def add_tool(self, tool: Tool) -> None:
        """Add a tool to the orchestrator."""
        self.tools[tool.name] = tool
        self._pipeline_built = False  # Need to rebuild pipeline
        logger.info(f"Added tool: {tool.name}")
    
    def _build_pipeline(self):
        """Build pipeline steps with current tools."""
        self.preprocessing_steps = [
            FileServerStep(self.tools),  # NEW: Check file server FIRST
            RAGRetrievalStep(self.retriever),
            PromptBuildStep(),
        ]
        self.generation_step = LLMGenerationStep(self.llm, self.enable_thinking)
        self._pipeline_built = True
    
    def run_preprocessing(self, context: Context) -> Context:
        """Run preprocessing steps (File Server, RAG, prompt building)."""
        if not self._pipeline_built:
            self._build_pipeline()
        
        for step in self.preprocessing_steps:
            try:
                context = step.process(context)
                logger.debug(f"Completed: {step.name}")
            except Exception as e:
                logger.error(f"Step {step.name} failed: {e}")
        
        return context
    
    def run_generation(self, context: Context) -> Context:
        """Run LLM generation step."""
        if not self._pipeline_built:
            self._build_pipeline()
        return self.generation_step.process(context)
    
    def process(self, query: str, chat_history: List[Dict] = None) -> AgentResponse:
        """Full pipeline: preprocess + generate."""
        context = Context(
            query=query,
            chat_history=chat_history or [],
            metadata={},
            debug_info=[]
        )
        
        # Preprocessing
        context = self.run_preprocessing(context)
        
        # Generation
        context = self.run_generation(context)
        
        # Build response
        return AgentResponse(
            answer=context.metadata.get("llm_response", ""),
            thinking=context.metadata.get("_internal_thinking", ""),
            intent=str(context.metadata.get("intent", "")),
            rag_used=bool(context.metadata.get("rag_context")),
            sources=context.metadata.get("sources", []),
            tool_used=context.metadata.get("tool_used", "")
        )
    
    def process_query(
        self, 
        query: str, 
        chat_history: List[Dict[str, str]] = None,
        metadata: Dict[str, Any] = None
    ) -> AgentResponse:
        """Process query with conversation memory support (alias for process)."""
        return self.process(query, chat_history)